[hparam]
data = /research/king3/ik_grp/wchen/amazon_review_data/min_4_Movies_and_TV_5/train_noex5_2019_12_12.pt
pre_word_vecs = /research/king3/ik_grp/wchen/amazon_review_data/min_4_Movies_and_TV_5wv_noex5_2019_12_12.npz
save_model = saved_models/movies_250/
log = logs/movies_250/
interval = 100
seed = 250
classes = 5


## Shared Memory
word_vec_size = 300
mem_size = 300
# LSTM
lstm = False
layer = 1
brnn = False
# CNN
kernel_size = 3,4,5
use_batch_norm = False


## Attention subnet
layer_c = 1
rnn_size_c = 300
hop = 2


# hyper-parameters
temp = 1.0
alpha = 1
beta = 2.5
enpy = -0.7


## Optimizition
batch_size = 32
epochs = 20
start_epoch = 1
param_init = 0.1
# rmsprop, sgd, adagrad, adadelta, adam, rmsprop
optim = rmsprop
max_grad_norm = 0.05
learning_rate = 0.0001

learning_rate_decay = 1
start_decay_at = 100

dropout = 0.5
extra_shuffle = False
weight_decay = 0


# visualize with tensorboardX
visualize = False
